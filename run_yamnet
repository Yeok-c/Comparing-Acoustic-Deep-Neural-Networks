import os, sys, glob
import numpy as np
import tensorflow as tf
import tensorflow_io as tfio
import random
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

sys.path.append('src')
import models.yamnet_tf2.yamnet_modified as yamnet
import models.yamnet_tf2.params as yamnet_params
params = yamnet_params.Params(sample_rate=16000, patch_hop_seconds=0.25)

# from dataload_utils.data_load import get_dataset, get_filenames_and_classnames_list
import dataload_utils.data_load as data_load
from dataload_utils.data_aug import mix_up

SEED = 42
random.seed(SEED)
tf.random.set_seed(SEED)

parent_dir = "C:\\Users\\User\\Documents\\cer_dataset_16k_flattened_resampled\\"
dataset_loader = data_load.Dataset_loader(parent_dir, params)
filenames_all = dataset_loader.__filenames_all__
classes = dataset_loader.__classes__
num_classes = dataset_loader.__num_classes__
print("classes: {}, num_classes: {}".format(classes, num_classes))

# To do real shuffling
AUTOTUNE = tf.data.AUTOTUNE
NUM_CLASSES = len(classes)
batch_size=64
random.shuffle(filenames_all)
filenames_train = filenames_all[:int(len(filenames_all)*0.7)]
filenames_eval = filenames_all[int(len(filenames_all)*0.7):int(len(filenames_all)*0.9)]
filenames_test = filenames_all[int(len(filenames_all)*0.9):]

# Training set preparation
dataset_aug = dataset_loader.get_dataset(filenames_train, augment=True)
train_dataset = dataset_aug.shuffle(batch_size*2).batch(batch_size) # Batch before doing mixup

# Mixup -
random.shuffle(filenames_train)
dataset_no_aug = dataset_loader.get_dataset(filenames_train, augment=False)

zipped_ds = tf.data.Dataset.zip((
    dataset_aug.shuffle(batch_size*2).batch(batch_size), 
    dataset_no_aug.shuffle(batch_size*2).batch(batch_size)
    ))

train_dataset = zipped_ds.map(
    map_func = lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.2), 
    num_parallel_calls=AUTOTUNE
    )

eval_dataset = dataset_loader.get_dataset(filenames_eval, augment=False).shuffle(batch_size*2).batch(batch_size)
test_dataset = dataset_loader.get_dataset(filenames_test, augment=False, flat_map=False).shuffle(batch_size*2)#.batch(batch_size)

train_dataset = train_dataset.cache().prefetch(AUTOTUNE)
eval_dataset = eval_dataset.cache().prefetch(AUTOTUNE)
test_dataset = test_dataset.cache().prefetch(AUTOTUNE)

# length = len(list(dataset_train_eval))
# print("Total length of dataset: ", length)

import datetime
MODEL_NAME='YAMNET'

# Paths
training_path = "./training/{}".format(datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

model_training_path = training_path + "/{}".format(MODEL_NAME)
ckp_path = model_training_path + "/checkpoints/cp.ckpt"
log_path = model_training_path + "/logs/fit"    
hd5_path = model_training_path + "/model.hd5"
cfm_path = model_training_path + "/confusion_matrix.png"

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=ckp_path,
                                                save_weights_only=True,
                                                verbose=1)
# Create a tensorboard callback                         
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_path, histogram_freq=1)

# Declare model
Yamnet = yamnet.Yamnet_modified(NUM_CLASSES)
yamnet_model = Yamnet.model()

yamnet_model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.CategoricalCrossentropy(), #from_logits=True
    metrics=['accuracy'],
)

# Fit model
yamnet_model.fit(train_dataset, validation_data = eval_dataset, epochs=40, 
    verbose=1, callbacks=[cp_callback,tensorboard_callback])

# Evaluate performance of model with test fold (that it wasn't trained on)
yamnet_model.load_weights(ckp_path)
loss, acc = yamnet_model.evaluate(test_dataset, verbose=2)
